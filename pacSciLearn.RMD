---
title: "PacSci Learn at Home"
favicon: "images/favicon_io"
output: 
  learnr::tutorial:
    css: "css/styles.css"
    theme: spacelab
    highlight: tango
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
options(warn=-1)
knitr::opts_chunk$set(echo = FALSE)
source("./global.R")

# Just get the data we need
sample_data_expected_n <- 100
sample_data <- d$gx$Primary_solid_Tumor %>% slice(1:95)
```


## Introduction

<!-- ### A Short Videoe -->
<!-- ![](https://www.youtube.com/watch?v=6KiLt2yt5xI) -->

![](https://www.youtube.com/watch?v=eG8S-KgjPFE&feature=youtu.be) 


<!-- TODO:  -->

<!-- -  Add text about the mission students are tasked with: helping distinguish high risk from low risk breast cancer tumor. -->

<!-- - Add definition on the terms used: e.g. algorithm, R, descriptive statistics, etc. -->


:::note
### Don't Skip Me! Navigating the site.
This site is set up to accompany you as you try your hand at some of the tools, technologies and science that power bioinformatics. We want to share a few quick tips before you get on your way.

1. On the left, you see tabs that will help you navigate through the sections in this site
2. There are multiple choice questions along the way and even optional coding. They are just to help you, we don't keep track :)
3. We said we don't keep track. That is true, but just to help you, the site remembers your answers, so you can pick up where you left off
4. Remember if you want to start fresh, you can always hit the **start over** button
5. There are "fyi" sections if you want to go deeper into the topics, but these are not required for exploring the site, so feel free to skip over them.
:::



Explore and have fun!

## Checking Data Integrity
<!-- (ACTIVITY VIDEO HERE) -->

In your mission to help breast cancer patients better quantify their risk, modern machine learning and artificial intelligence (AI) algorithms can give you `r fa("superpowers",   fill ="darkviolet")` super power `r fa("superpowers",   fill ="darkviolet")`. Think of yourself as a detective, and your `r fa("paw",   fill ="darkviolet")` four-legged assistant `r fa("paw",   fill ="darkviolet")` here would be the algorithm you use. As a detective, you would want your detective dog to have a sensitive and accurate nose. Likewise, as a bioinformatician, you want an algorithm that is sensitive and accurate.

But beside and accurate and sensitive nose, as a detective you want your dog to be well-trained. Imagine, you wanted your dog to pick up the scent of a dangerous chemical in a warehouse filled with different shipments. For that, you need to have trained your dog with the right scents or you'd have little chance of success. Similarly, your algorithm needs to have seen the right samples to be able to properly distinguish between cancers that may pose more risk to a patient than those that are more easily treatable. 

So before thinking about the algorithm, like any good detective, we want to check the authenticity of our evidence.  Here, data is our evidence. As a starter, we have been given a sample data-set for evaluation. We have been told it has come from `r sample_data_expected_n` patients. Data is organized in a table, with each row representing a patient and each column representing the activity level of a gene measured for that patient. Ok, let's ask our `r fa("robot",   fill ="darkviolet")` "assistant" `r fa("robot",   fill ="darkviolet")` to count the rows and columns in our sample data-set. 

<div id="rowcount-hint">
**Hint:**  Think which one of the buttons help us answer if all patients are represented in the data
</div>

```{r, echo=FALSE}
inputPanel(actionButton("nrow_pushed", "Count number of rows"))
inputPanel(actionButton("ncol_pushed", "Count number of columns"))
textOutput("row_count")
textOutput("col_count")
```

```{r, context="server"}
count_rows <- eventReactive(input$nrow_pushed, glue("There are {nrow(sample_data)} rows"))
count_cols <- eventReactive(input$ncol_pushed, glue("There are {ncol(sample_data)} columns"))
output$row_count <- renderText(count_rows())
output$col_count <- renderText(count_cols())
```


:::code
#### Use `r fa("r-project",   fill ="steelblue")` (Optional): 

The command to count the number of rows is `nrow`  and the data table you want to count is called **sample_data**. So how can you command R to count the number of rows in your data table? You pass your data table to it for counting. Passing can be done by `%>%` symbol, which is known as pipe. To count the number of rows, paste `sample_data %>% nrow` in the window below and then hit Enter on your keyboard or simply use the Run Code button.


```{r count_rows, exercise=TRUE, exercise.eval=FALSE}

```

Challenge: How would you count the number of columns if I tell you `ncol` is the command for counting columns?

```{r count_cols, exercise=TRUE, exercise.eval=FALSE}

```

```{r count_cols-hint}
sample_data %>% ncol
```
:::

<div id="npatient-hint">
**Hint:** We expect `r sample_data_expected_n` patients in the sample data we have received.
</div>

```{r data_integrity_qz}
quiz(
  question("Does our data have as many patients represented in it as we expected?",
    answer("Yes"),
    answer("No, we are missing 5", correct = TRUE),
    answer("No, we are missing 46"),
    answer("No, we have 3 more patients than expected")
  )
)
```



## Checking Data Quality 
<!-- `r fa("check-circle")` -->
<!-- (ACTIVITY VIDEO HERE) -->

### Descriptive Statistics
Let's say we have made sure that in fact the sample of training data we have fits the expected description. Now, we feel good about the integrity of our data! In other words, the training sample we have received are the ones we expected to receive, and there hasn't been any mix up during shipping. But what if the content we are shipped were low quality or wrong to begin with.  Going back to the analogy of a detective dog, in our training we want to have lots of good example and counter-example scents. If we want our dog to distinguish between the scent of an explosive material and a cleaning material that smells similarly, we need to have familiarized our dog with both. In other words, we want to have a **representative** sample.

With that in mind, let's first try to understand our data. Some basic descriptive statistics can help us evaluate how representative our sample is to the population for which we are interested in applying any insight gained from the data.

:::fyi
**Observational vs. experimental data:**
For testing scientific hypotheses, scientists almost always prefer to have access to data from experiments specifically designed to test their hypothesis. For example, to test efficacy of a treatment, scientific studies known and randomized clinical trial are designed with the specific aim of testing the hypothesis around efficacy of a treatment relative to an alternative. Volunteer patients are assigned to a new treatment or the alternative and outcomes are measured. As who is included in the study and what treatment is received is designed by the scientist, the data collected from such a study is known as experimental data. This type of data is hard to collect, but can help establish the highest level of evidence in support of a scientific conclusion. 

However, it is not always possible to design the experiments of interest. Let's say, you hypothesize that playing video games for more than 3 hours a day tends to hurt a student's grade. Ethically you cannot randomly assign a group of students to spend more than 3 hours playing video games and another group less than 3 hours. However, you can ask questions about students' typical number of hours spent playing video games and try to understand how that may relate to their grades.  The data collected in the way is called observational data, because your study has no say in which study plays how much video game. Rather you just observe.  Observational data, while easier to collect, have many pitfalls that if not considered could result in misleading conclusions. One critical element in evaluation of observational data, is understanding the attributes of the participants.

You can see that as our data here is observational, evaluation of data as you are engaging in this exercise is critically important!
:::


Below, you'll see the racial demographic of breast cancer patients in our data-set. 

```{r race, fig.width=10}
# d$clin %>% group_by(race) %>% tally %>% ggplot(aes(x="",y=n,fill=race)) + 
#   geom_col(width = 1) +  coord_polar("y", start=0) + theme_void() + 
#   ggsci::scale_fill_jama() + theme(legend.position = "bottom")

p1 <- plotly::plot_ly() %>% add_pie(data = dplyr::count(d$clin,race), labels = ~race, values = ~n)#,name = "race", domain = list(x = c(0, 0.4), y = c(0, 1)))
p1


# plotly::subplot(p1, p2, nrows = 1) %>%  plotly::layout(xaxis = list(domain = list(x = c(0, .49), y = c(0,1))))
```

and age distribution of the patients in the data data-set.

```{r age, fig.width=10}
g_age <- d$clin %>% mutate(age_at_diagnosis = age_at_diagnosis/365) %>% filter(!is.na(age_at_diagnosis)) %>% ggplot(aes(x=age_at_diagnosis)) + 
    geom_histogram(colour="white", binwidth = 10) + xlab("Age at diagnosis (years)") +
  scale_x_continuous(breaks = seq(20,100,5)) +
  scale_y_continuous(breaks = seq(0,500,20))

p2 <- g_age %>% ggplotly(.)
p2
```



Looking at the plots above, let's try to ask if our data is representative

<div id="hover-hint">
**Hint:**  mouse over `r fa("mouse-pointer",   fill ="black")`  the plots
</div>

```{r descriptive_statistics}

n_age30 <- ggplot_build(g_age)$data[[1]] %>% filter(x==30) %>% pull(y)
quiz(
  question("How many patients are between 25 and 35 years old?",
    answer(as.character(round(n_age30 * 2))),
    answer(as.character(round(n_age30 *  1.5))),
    answer(as.character(round(n_age30 *  1)), correct = TRUE),
    answer(as.character(round(n_age30 *  0.5)))
  ),
  question("Would this be a representative dataset if you wanted to learn about population of patients 30 or younger?",
    answer("Yes"),
    answer("No", correct = TRUE)
  )
)
```


## Finding Biological Signal 
<!-- `r fa("search")` -->

<!-- (ACTIVITY VIDEO HERE) -->

Let's say we have completed our data integrity and quality check and we are happy with the sample data. Now we are ready to try to `r fa("search",   fill ="darkviolet")` trace `r fa("search",   fill ="darkviolet")` biological signal that could allow us to tell the difference between a low and a high risk breast tumor.  Looking at medical journals, we notice two genes, **ESR1** and **PGR** that are linked to important biological processes. We also know that the processes linked to ESR1 and PGR can quite often be abnormal in breast tumors. That piques our interest. What can we learn if we explore how active these genes are? Since we have two genes of interest, we can plot the value of these two genes for each patient in our data in a 2-dimensional graph easily.  In this plot, each patient is represented by a single point. Here, imagine you are looking at the patients (dots) from above. The more a patient's ESR1 gene is active the more she moves to the right (along the horizontal axis) and the more her PGR gene is active, the higher that patient moves (along the vertical axis). 

For example, for the patient represented by the point circled in the graph below 

1. ESR1 value is `r d$gx$Primary_solid_Tumor %>% filter(id == "TCGA-BH-A1FU-01A-11R-A14D-07") %>% pull(ESR1) %>% log1p()` and 
2. PGR value is `r d$gx$Primary_solid_Tumor %>% filter(id == "TCGA-BH-A1FU-01A-11R-A14D-07") %>% pull(PGR) %>% log1p()`. 

Mouse over the point to confirm!

```{r}
highlight_id <- "TCGA-BH-A1FU-01A-11R-A14D-07"
ggplotly(ggplot(d$gx$Primary_solid_Tumor, aes(x = log1p(ESR1), log1p(PGR), label = id)) + geom_point(alpha = .5) + geom_point(data = d$gx$Primary_solid_Tumor %>% filter(id == highlight_id),size = 5, colour = "red", shape = 21) + xlab("ESR1") + ylab("PGR") )
```

Now, that we understand how this plot is constructed, let's take a closer look and see if we notice any pattern.

```{r ESR1_PGR_group_num}
quiz(
  question("If each dot represents one patient, do you see any groupings among how patients are arranged?",
    answer("Yes, I see 3 groups"),
    answer("Yes, I see 2 groups", correct = TRUE),
    answer("No, it looks like patients are evenly spread")
  )
)

```

As humans we are really good at recognizing patterns visually. Let's see how an algorithm would do? Let's show the data we just looked at to an algorithm made to find grouping (or clustering) in data.  There are many algorithms that are designed to do this.  Here, we are using an algorithm that does something you probably did without even thinking about it: putting two imaginary circles around each dense area, with the circles (or ovals) placed to best cover all the dots collectively. Let's try it out by pushing the button.


```{r, echo=FALSE}
inputPanel(actionButton("mclust_pushed", "Find and color two clusters"))
plotOutput("mclust_2g")
```

```{r, context="server"}
# TODO see if you can have a default one group prior to button push
do_mclust_2g <- eventReactive(input$mclust_pushed, {
  d$gx$Primary_solid_Tumor %>% 
    select(id, ESR1, PGR) %>% 
  left_join(x = ., y = d$molecular_subtype %>% select(id, mclust_esr1_pgr_g2),by="id") 
})

output$mclust_2g <- renderPlot({
  do_mclust_2g() %>% 
    ggplot(aes(x = log1p(ESR1), y = log1p(PGR), colour = mclust_esr1_pgr_g2)) +
    geom_point() + 
    ggsci::scale_color_jama() + 
    guides(colour=guide_legend(title="Algorithm identified groups")) + 
    theme(legend.position = "bottom") + 
    xlab("ESR1") + ylab("PGR")
  })
```

Ok, not bad for an algorithm `r fa("surprise",   fill ="darkviolet")`! It grouped our patients fairly well! But do these two groups tell us anything about patients' risk? For example, if a patient is in group 1 or group 2, are they likely to have different risks? We explore this question and more in the section on uncertainty. However, the topic is somewhat advanced and we left it as optional for those of you who find that exploration interesting. But whether or not you want to visit that topic, one thing we can take away is that we just listened in on some real biological signals (here measured as gene activities)! That allowed us to clearly see differences between patients' tumors at molecular levels, showing us the power of bioinformatics and modern tools of biology! In fact, approaches like this have led to development of algorithms that let us connect biological signals to clinical insight. In the next section we start exploring one such algorithm.

:::fyi
#### Unsupervised vs. supervised machine learning
Grouping or clustering is a form of machine learning called "unsupervised learning". Think of the example of training your dog to detect a scent among many. When you are showing your dog different scents and then reward the dog for recognizing the scent, in effect, you are "supervising" its learning. You know what is the right scent and you reward the dog when it's found. If you trained your algorithm this way, it'd be called a supervised learning approach, where you know exactly what your algorithm needs to find. Now imagine, you have scents that are similar but come from different chemicals, you as the trainer don't know which scent comes from which chemical, but your dogs nose being very sensitive, it may be able to distinguish subtle similarities and differences. If your dog could tell you that the sample scents came from different chemicals, like 2 barks for 2 different chemicals (we are stretching this analogy), then without you supervising your dog's learning, it was able to recognize the patterns. You can see that this would be a harder task for your dog. It is also a much harder (and more interesting) problem for algorithm developers to solve.

With unsupervised learning, you have tasks to completed

1. Find out how many different grouping there are in your sample data
2. Classify the data points into the groups

Providing the number of groups, could make the task quite a bit simpler, but that's often not known and that's why lots of research continues to find ways for algorithms to do both tasks simultaneously. 
:::


:::code
#### Use `r fa("r-project",   fill ="steelblue")` (Optional): 
Here are using an algorithm called **mclust** [1]. mclust is quite smart and has ways of doing both the task of finding the number of groups in your data as well as grouping your data accordingly. However, as smart as mclust is, there is no guarantee that it'll get the number of grouping right.  Try commanding R to cluster the data into 2, 3, 4 or 5 groups. Here `plot_g_cluster` is a function that accepts 2, 3, 4 or 5 as an input and generates an output plot based on the value of g. For example, `plot_g_cluster(g = 2)` should replicate the plot above. Note, mclust gave high quality scores to all these clustering results, but identified g=5 as the best option. In our case, this may not be scientifically meaningful so using scientific understanding, we may choose 2.

```{r change_mclust_gropuing, exercise=TRUE}
# Select g to be either 2, 3, 4, or 5
plot_g_cluster(g = )

```
:::


## Training and Prediction 1

<!-- (ACTIVITY VIDEO HERE) -->

In the previous section, we saw that although we found a real biological signal, based on the data we had, we couldn't be certain that the grouping we have found can inform us about the risk a tumor poses to a patient. Fortunately, we can always get help from work of others. In our case, a well-established algorithm for grouping of breast cancer, known as PAM50,is available for researcher.

:::fyi
#### PAM50 Algorithm:

PAM50 algorithm [2] takes its name from the 50 genes that it measures to group breast cancer tumors into 5 groups. These groups are named: 

1. Basal-like
2. HER2-Enriched
3. Luminal A
4. Luminal B
5. Normal-like

In our attempt at grouping breast tumors in the previous section, we used only 2 genes. PAM50 uses ESR1 and PGR genes, but also another 48 genes that are rigorously selected. The additional genes, informed by biological insight, create a grouping algorithm that has been shown through many independent investigations to provide information about relative differences in risks associated with breast cancer tumors.

**Important note:** Like many other tools and technologies, PAM50 algorithm as referred to here is used for researchers to gain insight about breast cancer. However its use in medical practice would require additional level of rigor that is beyond the scope what we are concerned with here.
:::

Given that we have PAM50 grouping available for our sample data, can we teach an algorithm to predict PAM50 grouping for a new sample? Given all the existing studies on PAM50 grouping, if our algorithm succeeds, we know that it then can help us with our mission: informing patients about the risk they are facing with their tumor. Of course, this is in effect reverse-engineering PAM50 algorithm, but it serves as a good exercise in showing how to teach an algorithm that is built on basic statistical concepts.  Here we are going to use only two such concepts:

1. Population median
2. Mathematical distance

Some of you may be familiar with one or both, but don't worry if you are not. We'll talk more about both.



### Median

Let's say you want to know what's a **typical** number of points your school's basketball team scores in any game. You watch 11 of their games and record each score. To find what's typical, you simply order the scores from smallest to largest and pick the middle number (6th number here) as that would be far away, in rank, from both the first and last ranked scores you recorded. 

You just calculated the median score for the 11 matches you watched!  If you had watched an even number of games (ex. 10, or 12) calculation would have been a bit different but the application is the same: *Median gives you a sense of what's typical*  

```{r meidan_qz}
quiz(
  question(glue("Given the sorted 11 numbers {paste(sort(rnorm11), collapse = \", \")}, can you spot the median?"),
    answer(glue("{min(rnorm11)}")),
     answer(glue("{signif(mean(rnorm11),2)}")),
    answer(glue("{quantile(rnorm11,probs = .5)}"), correct = TRUE),
    answer(glue("{max(rnorm11)}"))
  )
)

```


### Distance

The concept of mathematical distance is not much different from how you think about it in real life. In fact, you can probably answer the question below without any background information needed.
  

```{r distance_quiz}
list(
  A = list(x = 0, y= 0),
  B = list(x = 4, y= 3),
  C = list(x = 0, y= 3)
) %>% 
  bind_rows(.id = "label")  %>%
  ggplot(aes(x = x, y = y, label = label)) +
  geom_segment(x = 0, xend = 4, y = 0, yend = 3, linetype=2, colour = "blue")+
  geom_segment(x = 0, xend = 4, y = 3, yend = 3, linetype=2, colour = "red")+
  geom_segment(x = 0, xend = 0, y = 0, yend = 3, linetype=2, colour = "forestgreen") +
  geom_point(size = 12)+ 
  geom_text(size= 10, colour = "white")+
    xlim(0,4)+ ylim(0,3) + coord_fixed()

quiz(
  question("If you have three data points, A, B and C, which two are closest?",
           answer("A and B: Blue dashed line"),
           answer("A and C: Green dashed line", correct = TRUE),
           answer("B and C: Red dashed line")
  )
)
```

:::fyi
The distance we just used to quantify closeness is called Euclidean distance and is one of the ways closeness or similarity is measured in statistics and mathematics. There are many other measurements of closeness (or distance in a generic sense). For example, correlation and its variations are also distances. However, no matter what type of distance you use, the utility is typically to measure closeness or similarity and you can be sure distance from point A to B is always the same as distance measured from point B to A. 
:::

### Algorithm Training Strategy

```{r, include=FALSE}
toy_centroids <- list(
    low = list(x = 0, y= 0, type = "centroid"),
    high = list(x = 4, y= 3, type = "centroid"),
    median = list(x = 0, y= 3, type = "centroid"),
    "?" = list(x = 1, y= 2, type = "datapoint")
) %>% bind_rows(.id = "label")
```

Now that you know these two concepts, let's see how we can we use these two to build an algorithm that can tell us about the risk a particular patient's tumor poses her.

Let's work through a simple example. Let's say we know we have 3 groups of patients:

1. Patients with low risk tumors
2. Patients with medium risk tumors
3. Patients with high risk tumors

Let's say we measure activity of two important genes, Gene1 and Gene2 on a large number patients from each of the three groups.  We notice that:

1. Patients with low risk tumors **typically** have no activity for both gene 1 and gene 2
2. Patients with medium risk tumors **typically** have high gene 2 activity but no activity in gene 1 
3. Patients with high risk tumors **typically** have high activity in both genes measured

But how can we define typical level of gene activity for each group? Well, as we learned, median would certainly be one way to define typical level of gene activity. For each of the three groups we can calculate the median value of each of Gene1 and Gene2. That will give us three pairs of gene activity values. 

1. Patients with low risk tumors **typically** have `r glue("(Gene1 = {toy_centroids %>% filter(label == \"low\") %>% pull(x)}, Gene2 = {toy_centroids %>% filter(label == \"low\") %>% pull(y)})") `
2. Patients with medium risk tumors **typically** have `r glue("(Gene1 = {toy_centroids %>% filter(label == \"median\") %>% pull(x)}, Gene2 = {toy_centroids %>% filter(label == \"median\") %>% pull(y)})") `
3. Patients with high risk tumors **typically** have `r glue("(Gene1 = {toy_centroids %>% filter(label == \"high\") %>% pull(x)}, Gene2 = {toy_centroids %>% filter(label == \"high\") %>% pull(y)})") `

Now, we can visually represent each of the 3 typical patients (see the plot below). You may say, all we have done is picked 3 prototypical patients from our three groups. How can we use this to predict the risk a tumor poses a new patient? We will measure the activity level of Gene1 and Gene2 on the new patient. We will then find how close the measured activity is to the three prototypical patients. The closer the new patient to one of the three prototypical patients, the more similar her risk to that patient group.

```{r classification_quiz}
toy_centroids %>%
  ggplot(aes(x = x, y = y, label = label, shape = type)) +
  geom_segment(x = 1, xend = 0, y = 2, yend = 3, linetype=2, colour = "forestgreen")+
  geom_segment(x = 1, xend = 0, y = 2, yend = 0, linetype=2, colour = "red")+
  geom_segment(x = 1, xend = 4, y = 2, yend = 3, linetype=2, colour = "blue") +
  geom_point(aes(size = type, shape = type))+ 
  geom_text(size= 5, aes(colour = type) )+
  xlim(0,4)+ ylim(0,3) + coord_fixed() +
  xlab("Gene 1") + ylab("Gene 2") + 
  scale_size_manual(values = c(centroid = 25, datapoint = 10)) + 
  scale_color_manual(values = c(centroid = "white", datapoint = "white")) +
  scale_shape_manual(values = c(centroid = 16, datapoint = 15)) +
  theme(legend.position = "none")

quiz(
  question("Which group does the new sample that is shown by a question mark belong to?",
           answer("low"),
           answer("median", correct = TRUE),
           answer("high")
  )
)
```


So our strategy is clear:

1. Define the **prototypical patient** for each group as a patient showing the median level of activity for all genes measured in that group.
2. For a new patient, compare how close the level of activity of her tumor genes are to those of each of the **prototypical patients**.  The risk of the new patient is likely to be similar to the prototypical patient it is closest to.


## Training and Prediction 2

<!-- (ACTIVITY VIDEO HERE) -->

You now have a strategy to start building an algorithm that could help with the task of distinguishing between low and high risk breast cancer tumors. Instead of the made-up low, medium, and high categories, we just need to find groupings that are shown to be meaningful. PAM50 grouping, as we learned, gives us one such grouping. So our adjusted strategy is:

1. Define the **prototypical patient** for each **PAM50** group as a patient showing the median level of activity for all **50** genes measured in that group.
2. For a new patient, compare how close the level of activity of her tumor genes are to those of each of the **prototypical patients**.  The risk of the new patient is likely to be similar to the prototypical patient it is closet to.


:::note
Compare this strategy with that outline at the end previous section.  You notice, that our strategy is almost identical to the strategy we explored in the toy example in the last section. The major difference is that we now have 5 groups instead of the 3 groups (of low, median and high) in the toy example. So given a new sample, we would need to measure distance to 5 spots instead of 3 spots. That's not a major leap.

A bigger difference is that instead of 2 genes, we have 50 genes. We can plot 2-dimensional plots easily. However, we can't visualize 50 dimensions. However, distances are measured exactly the same way as in 2 dimensions, even when we can't visualize the data.
:::


### Training our algorithm

Fortunately, in our data, we know the PAM50 grouping for each tumor, so we could easily calculate the median activity level of each 50 genes within each group and store the value. This is all the teaching our algorithm will require! So going back to our toy example, this is similar to us finding the prototypical patient among each of low, medium and high risk groups. With the 5 prototypical patients gene activity level fixed, we are ready to group any new patient, by measuring how close her gene activity is to each of the prototypical patients.

But how do you measure closeness or distance when you have 50 genes instead of 2 genes? this sounds more complicated than it actually is. You can in fact do this with a bit of algebra. You can take us at our word or if interested read the fyi below and try it out for yourself.

```{r, include=FALSE}

pt_ids <- get_train_test_patient_ids()
mod <- get_model(patient_ids_train = pt_ids$patient_ids_train)
mod_calls <- mod(patient_ids_predict = pt_ids$patient_ids_train)
calls_evaluated <- evaluate_calls(model_calls = mod_calls)
overall_accuracy <- calls_evaluated$confusion_mat_stat %>% summarise_if(is.numeric,sum) %>% mutate(accuracy = (TP + TN)/(TP + TN + FP + FN)) %>% pull(accuracy)
overall_sensitivity <- calls_evaluated$confusion_mat_stat %>% summarise_if(is.numeric,sum) %>% mutate(sensitivity = (TP)/(TP + FN)) %>% pull(sensitivity)
overall_specificity <- calls_evaluated$confusion_mat_stat %>% summarise_if(is.numeric,sum) %>% mutate(specificity = (TN)/(TN + FP)) %>% pull(specificity)
```


:::fyi
**Measuring distances in higher dimensions:**

This may look or sound complex, but it is just simple algebra! Let's try a couple of examples and you'll see the pattern:

- 1-dimensional case: point A has coordinates $(x_A = 2)$, and B $(x_B = 5)$. Then distance between A and B is given by  $$\sqrt{(x_A - x_B)^2} = x_A - x_B = 5 - 2 = 3$$
- 2-dimensional case: point A has coordinates $(x_A = 2, y_A = 0)$ and point B $(x_B = 5, y_B = 4)$. Then the distance between A and B is given by $$\sqrt{(x_A - x_B)^2 + (y_A - y_B)^2} = \sqrt{(5 - 2)^2 + (4 - 0)^2} = 5$$
- 3-dimensional case: point A has coordinates $(x_A = 2, y_A = 0, z_A = 0)$ and point B $(x_B = 5, y_B = 4, z_B = 13)$. The the distance between A and B is given $$\sqrt{(x_A - x_B)^2 + (y_A - y_B)^2 + (z_A - z_B)^2} = \sqrt{3^2 + 4^2 + 13^2} = 12$$
- you can see the pattern for higher dimensions
:::


## Testing the Algorithm

<!-- (ACTIVITY VIDEO HERE) -->

Congratulations! We just trained an algorithm based on real patient data. Now we would want to know how well-trained our algorithm is, much like a detective might want to know about his 4-legged assistant. As we said in the beginning, we might want to know accuracy, sensitivity and specificity of algorithm.  So let's test how well our simple strategy that was just trained based on median values of gene activities does. 

```{r, echo=FALSE}
inputPanel(actionButton("accuracy_pushed", "Report overall algorithm accuracy"))
inputPanel(actionButton("sensitivity_pushed", "Report overall algorithm sensitivity"))
inputPanel(actionButton("specificity_pushed", "Report overall algorithm specificity"))

textOutput("accuracy_calculated")
textOutput("sensitivity_calculated")
textOutput("specificity_calculated")
```

```{r, context="server"}
get_accuracy <- eventReactive(input$accuracy_pushed, glue("Overall accuracy is {signif(overall_accuracy,2) * 100}%"))
get_sensitivity <- eventReactive(input$sensitivity_pushed, glue("Overall sensitivity is {signif(overall_sensitivity,2) * 100}%"))
get_specificity <- eventReactive(input$specificity_pushed, glue("Overall specificity is {signif(overall_specificity,2) * 100}%"))
output$accuracy_calculated <- renderText(get_accuracy())
output$sensitivity_calculated <- renderText(get_sensitivity())
output$specificity_calculated <- renderText(get_specificity())
```

For a simple training approach that relies on just median values, this is not bad at all!

:::fyi

**Performance within tumor subtypes:**

For understanding how different our algorithm performance is within each of the 5 categories, we can look at accuracy, sensitivity and specificity within each grouping:
```{r echo=F}
p <- calls_evaluated$confusion_mat_stat %>% 
  mutate(accuracy = (TP + TN)/(TP + TN + FP + FN)) %>%
  mutate(sensitivity = (TP)/(TP + FN)) %>%
  mutate(specificity = (TN)/(TN + FP)) %>%
  gather(metric,value,accuracy:specificity) %>% ggplot(aes(x = subtype, y = value, fill = value)) + geom_col() + coord_flip() + facet_wrap(~metric) +
  scale_fill_gradient(limits = c(0.2,1), low = "red", high = "forestgreen") +
  xlab(NULL) + ylab(NULL)

ggplotly(p)
```
We note that while sensitivity is getting less impressive within some groupings (Normal-like and Luminal A), overall our the performance of our trained algorithm remains acceptable.
:::

Before padding ourselves (and our computer) too much in the back, you might have noticed that the algorithm performance we are reporting tells us how well our algorithm predicts, when it has already seen the correct answer. This, as you can tell, would give us an overly optimistic evaluation.  So as a detective would when training his dog, we want to train our algorithm on some of the samples we have and save some for testing whether our algorithm learned well enough. The next exercise will allow you to explore how many examples do you need to show your algorithm to get its performance as high as it can go.


```{r, echo=FALSE}
sliderInput("n_train", "Number of examples to use in training:", min = 100, max = (d$gx$Primary_solid_Tumor %>% nrow) - 100, value = 500)
plotOutput("performance_per_n")
```

```{r, context="server"}

get_pt_ids <- reactive(get_train_test_patient_ids(n_train = input$n_train))
get_mod <- reactive(get_model(patient_ids_train = get_pt_ids()$patient_ids_train))
get_mod_calls <- reactive(get_mod()(patient_ids_predict = get_pt_ids()$patient_ids_test))
get_calls_evaluated <- reactive(evaluate_calls(model_calls = get_mod_calls()))



output$performance_per_n <- renderPlot({

  tmp_acc <- get_calls_evaluated()$confusion_mat_stat %>% 
    summarise_if(is.numeric,sum) %>% 
    mutate(accuracy = (TP + TN)/(TP + TN + FP + FN)) %>% 
    mutate(rest = 1-accuracy) %>% gather(key,value,accuracy:rest) %>%
    mutate(key = factor(key,levels = c("rest","accuracy")))
  
 p_acc <- tmp_acc %>% 
   ggplot(aes(x="",y=value,fill = key)) + 
   geom_col() + 
   coord_polar("y", start=0) +
   geom_text(data = tmp_acc %>% filter(key == "accuracy"), aes(y = value/2, label = glue("Accuracy: {signif(value,2) * 100}%") ), size=5, colour = "white") +
   theme_void() + 
   scale_fill_manual(values = c(rest = "white", accuracy = "darkviolet"))+ guides(fill=FALSE) +
   
   ggtitle(glue("Algorithm with {length(get_pt_ids()$patient_ids_train)} training sample"), subtitle = glue("Tested on {length(get_pt_ids()$patient_ids_test)} patients"))
 
 
 #-----------
   tmp_sens <- get_calls_evaluated()$confusion_mat_stat %>% 
    summarise_if(is.numeric,sum) %>% 
    mutate(sensitivity = (TP)/(TP + FN))%>% 
    mutate(rest = 1-sensitivity) %>% gather(key,value,sensitivity:rest) %>%
    mutate(key = factor(key,levels = c("rest","sensitivity")))
 
 p_sens <- tmp_sens %>% 
   ggplot(aes(x="",y=value,fill = key)) + 
   geom_col() + 
   coord_polar("y", start=0) +
   geom_text(data = tmp_sens %>% filter(key == "sensitivity"), aes(y = value/2, label = glue("Sensitivity: {signif(value,2) * 100}%") ), size=5, colour = "white")+
   theme_void() + 
   scale_fill_manual(values = c(rest = "white", sensitivity = "darkviolet")) + guides(fill=FALSE) +
   
   ggtitle(glue("Algorithm with {length(get_pt_ids()$patient_ids_train)} training sample"), subtitle = glue("Tested on {length(get_pt_ids()$patient_ids_test)} patients"))
 
 
  #-----------
   tmp_spec <- get_calls_evaluated()$confusion_mat_stat %>% 
    summarise_if(is.numeric,sum) %>% 
    mutate(specificity = (TN)/(TN + FP))%>% 
    mutate(rest = 1-specificity) %>% gather(key,value,specificity:rest) %>%
    mutate(key = factor(key,levels = c("rest","specificity")))
 
 p_spec <- tmp_spec %>% 
   ggplot(aes(x="",y=value,fill = key)) + 
   geom_col() + 
   coord_polar("y", start=0) +
   geom_text(data = tmp_spec %>% filter(key == "specificity"), aes(y = value/2, label = glue("Specificity: {signif(value,2) * 100}%") ), size=5, colour = "white") +
   theme_void() + 
   scale_fill_manual(values = c(rest = "white", specificity = "darkviolet"))  + guides(fill=FALSE) +
   
   ggtitle(glue("Algorithm with {length(get_pt_ids()$patient_ids_train)} training sample"), subtitle = glue("Tested on {length(get_pt_ids()$patient_ids_test)} patients"))
 
 p_acc + p_sens + p_spec
 
})
```


:::note

**How many examples to use for training?**

You may have noticed that changing number of examples shown to our algorithm does not make a noticeable change in how our algorithm does.  This suggests that with as few as 100 examples our algorithm can learn most of what it is ever going to learn! 

**Why?** Think about how our algorithm is "learning".  It simply calculates median values for the 5 PAM50 groups we are interested in. So if you show it 100 examples, the median it calculates comes from 100 values. In the context of gene activity values for the 5 PAM50 groups we are interested in, 100 values is typically good enough to give us fairly good estimation of median. This means that once we have the 100 samples shown, the median calculated may not be meaningfully different from median calculated when we double or triple the number of examples shown.
:::

### Can we make a smarter algorithm?

Think about your algorithm again as a detective dog, the stronger the ability of your dog's nose to tell the difference between subtle changes in scent, the more it has the **potential** to tell the difference between subtle changes in scent, but to actually do that, it will also need more example and counter examples showing it all the variations it is expected to recognize. 

Our current algorithm is a bit like a dog which has a descent sense of smell, but not an excellent one. In a dog-sense it can tell the difference between scents that are moderately different fairly well, and for learning that it won't need to many examples. That is enough for it to reach accuracy rate in low 90% range.  


## Wrap up

### Looking back

What an amazing journey we took together! Let's take a moment to think about where have been:

We started by talking about bionformatics as something like a superpower for biomedical scientists in their quest to make life-saving discoveries. It may have sounded like magic at the first, but for you that is not the case anymore. Let's list our main accomplishments:

- We used real patient data, evaluated its integrity and quality
- Discovered real patterns in genomic data that divided our patients' tumors into two groups
- Used (unsupervised) machine learning to explicitly divide the patients into two groups
- Explored two important statistical concepts, median and distance.
- Using just the two concept of median and distance, we built a (supervised) machine learning algorithm to group breast cancer tumors into 5 scientifically meaningful categories
- Evaluated the performance of our algorithm and even tackled the question of how many samples would be enough to train our algorithm

Wow! Even if you weren't able to follow all the concepts (which by the way is totally expected), you have definitely come a long way! At this point, you have way more than a basic idea of what bioinformatics is! 

We admit, some of these concept may have been a bit challenging. We know, finding medicine for breast cancer or discovering vaccines for infection diseases are even more challenging. But you are already better equipped than any generation before you (including us) to make discoveries in biomedical research that will transform lives. With bioinformatics as your superpower, your curiosity as your compass, and passion as your driving force, we have no doubt that you are up for the challenge.  We can't wait to see you and your `r fa("robot",   fill ="darkviolet")` "assistants" `r fa("robot",   fill ="darkviolet")` in the field, tacking on human diseases and improving health and healthcare for everyone!


### We want to acknkowledge

1. The Cancer Genome Atlas, ,[TCGA](https://www.cancer.gov/about-nci/organization/ccg/research/structural-genomics/tcga) and in particular the patients who participated in the initiative. Their generous act has and will continue to enable life-saving discoveries!
2. Pacific Science Center learn-at-home initiative and our BMS colleagues who forged our partnership with Pac Sci, allowing us to share with you this content
3. [leanR R package](https://rstudio.github.io/learnr/) for making it easy for data scientists to build educational site like this.

### References

1. Scrucca, Luca, et al. "mclust 5: clustering, classification and density estimation using Gaussian finite mixture models." The R journal 8.1 (2016): 289.

2. Parker, Joel S., et al. "Supervised risk predictor of breast cancer based on intrinsic subtypes." Journal of clinical oncology 27.8 (2009): 1160.

## Uncertainty (Advanced) 

<!-- (ACTIVITY VIDEO HERE) -->

:::note
The following section is not required for your progress through the site. This section deals with a relatively advanced topics of uncertainty in statistical inference and survival analysis. Feel free to skip to the next section.
:::

At this point, we have picked up a pattern in the our data: Looking at the activity levels of these two important genes, we see two distinct grouping of patients:

1. Patients with tumors that have both low ESR1 and low PGR activity
2. Patients with tumors that have both high ESR1 and high PGR gene activity

We expect that our patients with lower risk tumors to continue to survive breast cancer for longer time. Looking within each group, let's say we see that for the group with low ESR1 and PGR the percentage of patient who survival start dropping below 90% sooner than the other group, that may indicate the group with low ESR1 and PGR is associated with higher risk tumors. However, if we see no difference, we may conclude that we don't see evidence supporting that the biological grouping we saw can inform us about risk of a tumor. Let's take a look.

```{r,echo=F}

srv_fit <- d$molecular_subtype %>% mutate(status = if_else(vital_status == "Alive", 1, 2), time = as.numeric(days_to_last_followup)/365) %>%
  survfit(Surv(time, status) ~ mclust_esr1_pgr_g2, data = .)

srv_inference <- quantile(srv_fit,probs = .1)

srv_inference_table <- srv_inference %>% map(.x = ., .f = function(x) data.frame(x) %>% mutate(g = rownames(.))) %>% bind_rows(.id = "param") %>% spread(param,X10) %>%
  select(g, quantile, lower, upper) 

srv_inference_table %>% 
  select(g, quantile) %>%
  mutate_if(is.numeric, function(x) signif(x,2))%>%
  rename("Group" = g, "Years until survival falls below 90%" = quantile) %>% knitr::kable(.)

# g1 <-ggplot(d$gx$Primary_solid_Tumor, aes(x = 0, y = log1p(ESR1))) +  geom_dotplot(binaxis = "y", stackdir = "center", fill = "white", binwidth = 1/9) + geom_violin(fill = NA, linetype = 2, colour = "blue") +xlim(-1, 1)

# g2 <- ggplot(d$gx$Primary_solid_Tumor, aes(x = 0, y = log1p(PGR)))  + geom_dotplot(binaxis = "y", stackdir = "center", fill = "white", binwidth = 1/6.5) + geom_violin(fill = NA, linetype = 2, colour = "blue") + xlim(-1, 1)
# 
# g1 + g2
# ggplot(d$gx$Primary_solid_Tumor, aes(x = log1p(ESR1), log1p(PGR) )) +geom_density_2d_filled(alpha=.3) + geom_point(alpha=.2)

```

<div id="srv-hint">
**Hint:**  Remember a higher risk means it takes less time for the percentage of patients who have survival to fall below 90%
</div>
```{r srv_quiz}
quiz(
  question("Looking at the survival estimate, which group appears to have a higher risk",
    answer("Group 2"),
    answer("Group 1", correct = TRUE)
  )
)

```

You may have noticed that the quiz above is careful to use the word "**appears** to have a higher risk".  The reason is that we haven't accounted for uncertainty in our evaluation and every time you make an observation in a sample of a population (and here we are only looking at small fraction of all breast cancer patients), there is a chance that our observation is coincidence. There statistical techniques that allow us to quantify the uncertainty in our observations. A commonly used one is called a confidence interval. An intuitive definition for a 95% confidence interval is the range of typical values we expect for an observation based on data (the precise definition is less intuitive).  Now, let's look at our table of survival with 95% interval included.

```{r}
srv_inference_table %>% 
  mutate_if(is.numeric, function(x) signif(x,2))%>%
  rename("Group" = g, "Years until survival falls below 90%" = quantile) %>% knitr::kable(.)
```

Now, we can see that although we saw a trend in the data, our data does not even allow us to estimate the upper end of confidence interval for each group. In fact, we are unable to say that the difference we see between the two groups is not just a coincidence. As a bioinformatician, there are many times that you find a signal that you can't say for sure if it relates to questions your care about, even when the signal is real (and not noise).  As you train you algorithm, you want to make sure it is rooted in math and statistics because calling out false signals is key to finding the real signals.

Coming back to the signal we found, is the grouping we found informative for our question? We can't tell based on the analyses we looked at. 


